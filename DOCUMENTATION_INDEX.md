# SAM 2 Documentation Index

This file provides a master index to all documentation about the SAM 2 codebase.

## Start Here

- **QUICK_REFERENCE.md** - Quick lookup guide for common tasks (5 min read)
- **CODEBASE_OVERVIEW.md** - Comprehensive architecture and component guide (30 min read)

## Official Documentation (in Repository)

| Document | Purpose | Location |
|----------|---------|----------|
| **README.md** | Project overview, quick start | Root |
| **INSTALL.md** | Detailed installation instructions | Root |
| **RELEASE_NOTES.md** | Version history and changes | Root |
| **CONTRIBUTING.md** | How to contribute | Root |
| **CODE_OF_CONDUCT.md** | Community standards | Root |
| **training/README.md** | Training & fine-tuning guide | training/ |
| **demo/README.md** | Web demo setup (GraphQL) | demo/ |
| **image_demo/README.md** | Image demo setup (JSON API) | image_demo/ |
| **tools/README.md** | Evaluation tools documentation | tools/ |
| **sav_dataset/README.md** | SA-V dataset information | sav_dataset/ |

## Generated Documentation

| Document | Purpose | Generated By Claude |
|----------|---------|---------------------|
| **CODEBASE_OVERVIEW.md** | Detailed architecture analysis | Yes |
| **QUICK_REFERENCE.md** | Quick lookup and patterns | Yes |

## Code Examples

### Jupyter Notebooks (in `/notebooks/`)
1. **image_predictor_example.ipynb** - Static image segmentation with interactive prompts
2. **video_predictor_example.ipynb** - Video tracking with multi-object support
3. **automatic_mask_generator_example.ipynb** - Dense unsupervised mask generation

Also available on Google Colab for easy access.

## Architecture Documentation

### Core Components
- **Image Encoder** - `sam2/modeling/backbones/hieradet.py` (Hierarchical architecture)
- **Memory System** - `sam2/modeling/memory_encoder.py` & `memory_attention.py` (Video tracking)
- **Mask Decoder** - `sam2/modeling/sam/mask_decoder.py` (From original SAM)
- **Transformer** - `sam2/modeling/sam/transformer.py` (Attention mechanisms)

### Full Architecture Diagram
See `assets/model_diagram.png` for visual overview

## API Reference

### Image Segmentation
```python
from sam2.sam2_image_predictor import SAM2ImagePredictor
from sam2.build_sam import build_sam2

model = build_sam2(config_file="...", ckpt_path="...")
predictor = SAM2ImagePredictor(model)
```
See: QUICK_REFERENCE.md section "Image Prediction"

### Video Tracking
```python
from sam2.sam2_video_predictor import SAM2VideoPredictor
from sam2.build_sam import build_sam2_video_predictor

predictor = build_sam2_video_predictor(config_file="...", ckpt_path="...")
state = predictor.init_state(video_path)
```
See: QUICK_REFERENCE.md section "Video Tracking"

### Automatic Mask Generation
```python
from sam2.automatic_mask_generator import SAM2AutomaticMaskGenerator

mask_gen = SAM2AutomaticMaskGenerator(model)
masks = mask_gen.generate(image)
```
See: QUICK_REFERENCE.md section "Automatic Masks"

## Configuration System

All configs use Hydra YAML format:
- **Model Configs**: `sam2/configs/sam2/` and `sam2/configs/sam2.1/`
- **Training Configs**: `sam2/configs/sam2.1_training/`
- **Override Examples**: See QUICK_REFERENCE.md section "Configuration Files"

## Dataset Documentation

### SA-V Dataset (Segment Anything Video)
- 51K videos with 643K segmentation masks
- Used for training SAM 2
- Details: `sav_dataset/README.md`

### Supported Datasets for Training
- **SA-1B** - Image dataset
- **SA-V** - Video dataset
- **MOSE** - Video segmentation benchmark
- **DAVIS** - Video segmentation benchmark

## Training Documentation

Complete training pipeline:
- **launcher**: `training/train.py` - Single/multi-node support
- **main loop**: `training/trainer.py` - Training orchestration
- **datasets**: `training/dataset/` - Data loading
- **losses**: `training/loss_fns.py` - Loss functions
- **optimization**: `training/optimizer.py` - Optimizer configuration

See: `training/README.md` for detailed guide

## Deployment Documentation

### Web Demos
1. **Full Interactive Demo** (`demo/`)
   - Flask + GraphQL backend
   - React + Vite frontend
   - Docker containerization
   
2. **Lightweight Image Demo** (`image_demo/`)
   - Flask JSON API backend
   - React + Vite frontend
   - Simple REST interface

See respective README.md files

## File Lookup by Task

Want to find code for...

| Task | Primary File | Secondary | Config |
|------|--------------|-----------|--------|
| Load models | `build_sam.py` | | `sam2/configs/` |
| Segment images | `sam2_image_predictor.py` | `build_sam.py` | `sam2/configs/sam2.1/` |
| Track videos | `sam2_video_predictor.py` | `build_sam.py` | `sam2/configs/sam2.1/` |
| Auto masks | `automatic_mask_generator.py` | | |
| Core model | `modeling/sam2_base.py` | | `sam2/configs/` |
| Image encoder | `modeling/backbones/hieradet.py` | | |
| Video memory | `modeling/memory_encoder.py` | `modeling/memory_attention.py` | |
| Train model | `training/train.py` | `training/trainer.py` | `training/configs/` |
| Evaluate | `sav_dataset/sav_evaluator.py` | `tools/vos_inference.py` | |

## Key Metrics & Performance

### Model Sizes (SAM 2.1)
| Model | Parameters | Speed | Best For |
|-------|-----------|-------|----------|
| tiny | 38.9M | 91.2 FPS | Mobile/Real-time |
| small | 46M | 84.8 FPS | Real-time |
| base+ | 80.8M | 64.1 FPS | Balanced |
| large | 224.4M | 39.5 FPS | Best accuracy |

### Benchmarks
- **SA-V test**: 79.5 J&F (large model)
- **MOSE validation**: 74.6 J&F (large model)
- **LVOS v2**: 80.6 J&F (large model)

## Common Issues & Solutions

### Installation
See: `INSTALL.md` for detailed troubleshooting

### Performance
See: QUICK_REFERENCE.md section "Debugging Tips"

### Training
See: `training/README.md`

### Deployment
See: `demo/README.md` or `image_demo/README.md`

## Development Workflow

1. **Understanding Architecture** → Start with CODEBASE_OVERVIEW.md sections 2-4
2. **Running Examples** → Execute notebooks in `/notebooks/`
3. **Using APIs** → QUICK_REFERENCE.md Key Classes section
4. **Customization** → Read relevant config YAML files
5. **Debugging** → QUICK_REFERENCE.md Debugging Tips

## Version Information

- **Current**: SAM 2.1 (September 2024) - Use this
- **Legacy**: SAM 2 (July 2024) - Backward compatible
- **Original**: SAM 1.0 - Separate codebase

Current codebase supports all versions via config selection.

## Getting Help

1. Check relevant README.md in INSTALL.md, training/, demo/, etc.
2. Look at examples in `/notebooks/`
3. Review QUICK_REFERENCE.md for common patterns
4. Check CODEBASE_OVERVIEW.md for architecture details
5. See official repo: https://github.com/facebookresearch/sam2

## Repository Structure

```
sam2/
├── DOCUMENTATION_INDEX.md     <- You are here
├── QUICK_REFERENCE.md          <- Start here for quick lookup
├── CODEBASE_OVERVIEW.md        <- Full architecture details
├── README.md                   <- Project overview
├── INSTALL.md                  <- Installation guide
├── setup.py                    <- Installation config
│
├── sam2/                       <- Core package
│   ├── build_sam.py            <- Factory functions
│   ├── sam2_image_predictor.py <- Image API
│   ├── sam2_video_predictor.py <- Video API
│   ├── modeling/               <- Neural network
│   └── configs/                <- Model configs
│
├── training/                   <- Training code
│   ├── train.py                <- Launch script
│   ├── trainer.py              <- Training loop
│   ├── dataset/                <- Data loading
│   └── configs/                <- Training configs
│
├── notebooks/                  <- Examples
│   ├── image_predictor_example.ipynb
│   ├── video_predictor_example.ipynb
│   └── automatic_mask_generator_example.ipynb
│
├── demo/                       <- Full web demo
├── image_demo/                 <- Simple JSON demo
├── tools/                      <- Evaluation tools
└── sav_dataset/                <- Dataset utilities
```

---

**Last Updated**: November 2024
**For Questions**: Check README.md or CODEBASE_OVERVIEW.md
**Quick Lookup**: Use QUICK_REFERENCE.md
